# GRPO Research Summary

**Date:** January 19, 2026  
**Task:** Browse GitHub for working GRPO training implementations  
**Status:** âœ… Complete

---

## What Was Delivered

### 1. **Comprehensive Research Document** 
ðŸ“„ `GRPO_IMPLEMENTATIONS_RESEARCH.md` (27KB)

A complete analysis of GRPO implementations across GitHub including:
- 20+ repositories analyzed
- 6 implementations studied in detail (HuggingFace TRL, nanoGRPO, Google Tunix, etc.)
- Full code examples from production implementations
- Specific recommendations for GLM-Image architecture
- Common pitfalls and solutions
- Testing strategies
- Memory optimization techniques

### 2. **Quick Reference Guide**
ðŸ“„ `GRPO_QUICK_REFERENCE.md` (8KB)

A condensed, practical guide for quick access including:
- Core GRPO formula and concepts
- Top 3 implementations to study
- Step-by-step implementation guide
- Code snippets for key components
- Configuration template
- Testing checklist
- GLM-Image specific adaptations

---

## Key Discoveries

### What is GRPO?

**Group Relative Policy Optimization** is a reinforcement learning algorithm from the DeepSeekMath paper that simplifies PPO by:
- Removing the need for a value function/critic network
- Computing advantages as z-scores within sample groups
- Using PPO-style clipped loss for stability

**Core Innovation:**
```
Advantage = (reward - mean_reward_in_group) / (std_reward_in_group + Îµ)
```

This makes it:
- âœ… Simpler than PPO (no critic needed)
- âœ… More memory-efficient
- âœ… Easier to implement and debug
- âœ… Works well for language/reasoning tasks

### Top 3 Implementations Found

#### 1. HuggingFace TRL (Production)
- **Best for:** Implementing GRPO in GLM-Training
- **Why:** Production-ready, actively maintained, full-featured
- **URL:** https://github.com/huggingface/trl
- **File:** `trl/trainer/grpo_trainer.py`
- **Features:** Multi-GPU, vLLM, multiple rewards, async support

#### 2. nanoGRPO (Educational)
- **Best for:** Understanding GRPO concepts
- **Why:** Clean, minimal, single-file implementation
- **URL:** https://github.com/joey00072/nanoGRPO
- **File:** `grpo.py` (just 1 file!)
- **Features:** Works on 8GB GPU, clear code structure

#### 3. Google Tunix (Research)
- **Best for:** High-performance implementation
- **Why:** Google-quality, JAX-based, optimized
- **URL:** https://github.com/google/tunix
- **File:** `tunix/rl/grpo/grpo_learner.py`
- **Features:** TPU support, modular design

---

## Current GLM-Training Status

### What's Working
âœ… Basic reward computation with multiple metrics  
âœ… Multi-GPU distributed training  
âœ… Separate AR and DiT training  
âœ… T2I and I2I modes  
âœ… Simple reconstruction loss training

### What's Missing (Identified from Research)
âŒ **No log probability tracking** - Can't compute policy gradients  
âŒ **No reference model** - Can't compute KL divergence correctly  
âŒ **No group-based sampling** - Generates one sample per prompt  
âŒ **No advantage computation** - No z-score within groups  
âŒ **No PPO clipping** - Using reconstruction loss instead  

### Gap Analysis
The current `reward_trainer.py` was reverted to a simpler version after complex GRPO attempts failed. The research reveals exactly what patterns need to be adopted from working implementations.

---

## Implementation Roadmap

### Phase 1: Core GRPO (High Priority)
Based on HuggingFace TRL patterns:

1. **Reference Model**
   ```python
   ref_model = copy.deepcopy(model)
   for param in ref_model.parameters():
       param.requires_grad = False
   ```

2. **Log Probability Tracking**
   ```python
   def get_per_token_logprobs(model, input_ids):
       logits = model(input_ids).logits
       log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)
       return torch.gather(log_probs, -1, input_ids[:, 1:].unsqueeze(-1))
   ```

3. **Group-Based Generation**
   ```python
   # Generate 4-8 samples per prompt
   for _ in range(group_size):
       completion = model.generate(prompt)
       completions.append(completion)
   ```

4. **Advantage Computation**
   ```python
   # Z-score within each group
   rewards = rewards.view(batch_size, group_size)
   advantages = (rewards - rewards.mean(1, keepdim=True)) / \
                (rewards.std(1, keepdim=True) + 1e-6)
   ```

5. **GRPO Loss**
   ```python
   # PPO-style clipped loss + KL penalty
   ratio = torch.exp(new_logprobs - old_logprobs)
   clipped = torch.clamp(ratio, 1-epsilon, 1+epsilon)
   loss = -torch.min(ratio * advantage, clipped * advantage)
   loss += beta * kl_divergence
   ```

### Phase 2: Memory Optimization (Medium Priority)
Based on nanoGRPO patterns:

1. **Micro-batching** - Process groups in smaller chunks
2. **CPU Offloading** - Move data to CPU when not in use
3. **Gradient Accumulation** - Larger effective batch sizes
4. **Empty Cache** - Clear GPU memory between steps

### Phase 3: Advanced Features (Low Priority)
Based on production implementations:

1. **vLLM Integration** - Faster generation
2. **Multiple Reward Models** - Combine different rewards
3. **Async Rewards** - Parallel reward computation
4. **Curriculum Learning** - Progressive difficulty

---

## Configuration Template

For GLM-Training's YAML config:

```yaml
reward:
  enabled: true
  grpo:
    # Core GRPO settings
    num_samples: 4           # Completions per prompt (4-8 typical)
    clip_range: 0.2          # PPO clip epsilon (0.1-0.3)
    kl_coef: 0.05            # KL penalty coefficient (0.01-0.1)
    normalize_advantages: true
    advantage_epsilon: 1e-6
    
  # Vision-specific rewards for GLM-Image
  metrics:
    lpips: 0.4              # Perceptual similarity
    ssim: 0.2               # Structural similarity
    text_accuracy: 0.3      # OCR-based text matching
    aesthetic: 0.1          # Aesthetic quality
    
  # Memory optimization
  micro_batch_size: 2       # Process 2 samples at a time
  offload_ref_model: true   # Move reference to CPU when not used
```

---

## Key Code Patterns

### Pattern 1: Training Step Structure
From HuggingFace TRL:

```python
def train_step(self, batch):
    # 1. Generate multiple completions (no gradients)
    with torch.no_grad():
        completions, old_log_probs = self.generate_group(
            batch["prompts"], num_samples=4
        )
    
    # 2. Compute rewards
    rewards = self.reward_calculator(completions, batch["targets"])
    
    # 3. Compute advantages (z-score per group)
    advantages = self.compute_advantages(rewards, group_size=4)
    
    # 4. Compute loss (with gradients)
    new_log_probs = self.get_log_probs(self.model, completions)
    ref_log_probs = self.get_log_probs(self.ref_model, completions)
    loss = self.grpo_loss(new_log_probs, old_log_probs, 
                          ref_log_probs, advantages)
    
    # 5. Optimize
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()
```

### Pattern 2: Memory-Efficient Training
From nanoGRPO:

```python
# Split groups into micro-batches
for micro_batch in split(group, micro_size):
    loss = compute_loss(micro_batch)
    loss.backward()
    torch.cuda.empty_cache()

optimizer.step()
optimizer.zero_grad()
```

### Pattern 3: GLM-Image Adaptation
For vision + diffusion models:

```python
# AR model generates visual tokens (trackable)
ar_outputs = model.ar_model.generate(
    prompts, return_log_probs=True
)

# DiT decoder converts to images (deterministic)
images = model.dit_model.decode(ar_outputs.tokens)

# Use AR log probs for policy gradient
# DiT doesn't need gradients (deterministic)
return images, ar_outputs.log_probs
```

---

## Testing Strategy

### Unit Tests
```python
def test_advantage_computation():
    """Test z-score within groups."""
    rewards = torch.tensor([[1, 2, 3, 4], [5, 5, 5, 5]])
    advantages = compute_advantages(rewards.flatten(), group_size=4)
    
    # Group 1 should have variance
    assert advantages[:4].std() > 0
    
    # Group 2 should be zero (constant rewards)
    assert torch.allclose(advantages[4:], torch.zeros(4))

def test_ppo_clipping():
    """Test PPO clip works."""
    # When ratio > 1.2, should be clipped
    new_lp = torch.tensor([1.0])  # ratio = exp(1.0) â‰ˆ 2.7
    old_lp = torch.tensor([0.0])
    advantage = torch.tensor([1.0])
    
    loss = grpo_loss(new_lp, old_lp, advantage, clip=0.2)
    
    # Should be clipped to 1.2
    assert loss < advantage * 2.7
```

### Integration Tests
```python
def test_full_pipeline():
    """Test complete training step."""
    trainer = RewardTrainer(config)
    batch = {
        "prompts": ["test prompt"],
        "target_images": [test_image],
    }
    
    metrics = trainer.train_step(batch)
    
    assert "loss" in metrics
    assert "reward" in metrics
    assert metrics["loss"] > 0
```

### Memory Tests
```python
def test_memory_fits():
    """Ensure training fits in GPU memory."""
    torch.cuda.reset_peak_memory_stats()
    
    for _ in range(10):
        trainer.train_step(batch)
    
    peak = torch.cuda.max_memory_allocated()
    assert peak < 80 * 1024**3  # 80GB
```

---

## Common Pitfalls Identified

From analyzing failed implementations:

### âŒ Pitfall 1: Global Normalization
```python
# WRONG: Normalizes across all rewards
advantages = (rewards - rewards.mean()) / rewards.std()
```
**Fix:** Normalize within each group
```python
# CORRECT: Z-score per group
rewards = rewards.view(batch_size, group_size)
advantages = (rewards - rewards.mean(1, keepdim=True)) / \
             rewards.std(1, keepdim=True)
```

### âŒ Pitfall 2: Gradient Tracking
```python
# WRONG: Gradients through generation
completions = model.generate(prompts)
loss = compute_loss(completions)  # Breaks!
```
**Fix:** Generate without gradients, recompute for training
```python
# CORRECT
with torch.no_grad():
    completions, old_lp = model.generate(prompts)
new_lp = compute_logprobs(model, completions)  # With gradients
```

### âŒ Pitfall 3: Missing Reference Model
```python
# WRONG: KL with old policy
kl = new_log_probs - old_log_probs
```
**Fix:** Use frozen reference model
```python
# CORRECT
ref_log_probs = compute_logprobs(ref_model, completions)
kl = new_log_probs - ref_log_probs
```

---

## Resources Created

### For Quick Start
- **GRPO_QUICK_REFERENCE.md** - 8KB condensed guide
  - Core concepts and formulas
  - Step-by-step implementation
  - Code snippets
  - Testing checklist

### For Deep Dive
- **GRPO_IMPLEMENTATIONS_RESEARCH.md** - 27KB comprehensive analysis
  - 20+ repositories analyzed
  - Detailed code examples
  - Architecture patterns
  - Recommendations for GLM-Training

### External References
- **HuggingFace TRL Docs:** https://huggingface.co/docs/trl/grpo_trainer
- **nanoGRPO Repo:** https://github.com/joey00072/nanoGRPO
- **DeepSeekMath Paper:** https://arxiv.org/abs/2402.03300
- **DeepSeek R1 Paper:** https://arxiv.org/abs/2501.12948

---

## Next Steps

### For Immediate Action
1. **Study the implementations:**
   - Read through HuggingFace TRL's `grpo_trainer.py`
   - Study nanoGRPO's `grpo.py` for concepts

2. **Review current code:**
   - Check `glm_training/trainers/reward_trainer.py`
   - Identify specific changes needed
   - Plan minimal surgical updates

3. **Start with Phase 1:**
   - Add reference model creation
   - Implement log probability tracking
   - Add group-based generation
   - Implement advantage computation
   - Replace loss with GRPO loss

### For Testing
1. Create unit tests for components
2. Test on small model first
3. Validate memory usage
4. Benchmark against simple reward training

### For Deployment
1. Update configuration files
2. Update documentation
3. Add examples for T2I and I2I
4. Create migration guide

---

## Success Metrics

How to know GRPO is working:

âœ… **Advantages have zero mean per group**  
âœ… **PPO clipping activates (ratio != policy_loss)**  
âœ… **KL divergence is positive and bounded**  
âœ… **Training loss decreases**  
âœ… **Rewards increase over time**  
âœ… **Memory usage is stable**  
âœ… **Generated images improve quality**

---

## Conclusion

This research provides a complete roadmap for implementing proper GRPO in GLM-Training:

1. âœ… **Best practices identified** from production code
2. âœ… **Specific patterns documented** with code examples
3. âœ… **Current gaps analyzed** with clear solutions
4. âœ… **Implementation roadmap** with phases
5. âœ… **Testing strategies** provided
6. âœ… **Common pitfalls** documented with fixes

The team now has everything needed to implement GRPO correctly, learning from the best implementations available on GitHub.

---

**Research Completed:** January 19, 2026  
**Documents Created:** 3 (Research, Quick Reference, Summary)  
**Repositories Analyzed:** 20+  
**Code Examples:** 10+  
**Ready for Implementation:** âœ… Yes
