# Text-to-Image Training Configuration

# Model Configuration
model:
  name: "zai-org/GLM-Image"
  component: "dit"  # Options: "ar" (autoregressive only), "dit" (decoder only), "both"
  torch_dtype: "bfloat16"
  device_map: "auto"
  
  # Autoregressive model settings
  ar:
    temperature: 0.9
    top_p: 0.75
    do_sample: true
    
  # DiT decoder settings
  dit:
    num_inference_steps: 50
    guidance_scale: 1.5

# Training Configuration
training:
  mode: "t2i"  # text-to-image mode
  
  # Basic training parameters
  # Note: batch_size must be 1 due to AR model limitations in the GLM-Image pipeline.
  # Use gradient_accumulation_steps to achieve effective larger batch sizes.
  batch_size: 1
  gradient_accumulation_steps: 8  # Effective batch size = 1 * 8 = 8
  num_epochs: 2
  max_steps: null  # Set to override num_epochs
  
  # Learning rates
  learning_rate: !!float 1e-5
  ar_learning_rate: !!float 5e-6  # Separate LR for autoregressive model
  dit_learning_rate: !!float 1e-5  # Separate LR for DiT decoder
  
  # Optimizer settings
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: !!float 1e-8
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Learning rate schedule
  lr_scheduler: "cosine"
  warmup_steps: 50
  
  # Memory optimization
  gradient_checkpointing: true
  mixed_precision: "bf16"  # Options: "no", "fp16", "bf16"
  
  # Reward-based training
  use_reward: true
  reward_frequency: 1  # Compute rewards every N steps

# Reward Configuration
reward:
  enabled: true
  target_images_dir: "data/t2i/target_images"
  
  # Reward metrics and weights
  metrics:
    lpips: 0.4  # Perceptual similarity
    aesthetic: 0.3  # Aesthetic quality score
    text_accuracy: 0.3  # Text rendering accuracy
    
  # GRPO (Group Relative Policy Optimization) settings
  grpo:
    num_samples: 4  # Number of samples per prompt for GRPO
    kl_coef: 0.05  # KL divergence coefficient
    clip_range: 0.2  # PPO-style clipping range
    
  # Component-specific reward settings
  ar_reward_weight: 1.0  # Weight for AR component rewards
  dit_reward_weight: 1.0  # Weight for DiT component rewards

# Data Configuration
data:
  # Dataset paths
  prompts_file: "data/t2i/prompts.txt"
  target_images_dir: "data/t2i/target_images"
  
  # Data loading
  num_workers: 4
  shuffle: true
  pin_memory: true
  
  # Data augmentation for t2i
  augmentation:
    enabled: false
    
  # Image settings
  image_size:
    height: 1024  # Must be divisible by 32
    width: 1024   # Must be divisible by 32

# Distributed Training Configuration
distributed:
  enabled: false
  backend: "nccl"  # Options: "nccl", "gloo"
  find_unused_parameters: false

# Logging Configuration
logging:
  log_dir: "logs"
  log_interval: 10  # Log every N steps
  save_interval: 500  # Save checkpoint every N steps
  eval_interval: 100  # Evaluate every N steps
  
  # Logging backends
  use_tensorboard: true
  use_wandb: false
  wandb_project: "glm-image-training"
  wandb_entity: null
  
  # Image logging
  log_images: true
  num_log_images: 4  # Number of images to log

# Checkpoint Configuration
checkpoint:
  save_dir: "checkpoints"
  resume_from: null  # Path to checkpoint to resume from
  save_total_limit: 3  # Keep only N most recent checkpoints
  save_optimizer: true

# Evaluation Configuration
evaluation:
  enabled: true
  eval_prompts_file: "data/t2i/eval_prompts.txt"
  eval_target_images_dir: "data/t2i/eval_target_images"
  num_eval_samples: 2

# Seed for reproducibility
seed: 42
